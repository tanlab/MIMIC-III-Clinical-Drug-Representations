{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ffe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np \n",
    "import keras\n",
    "#from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, concatenate, Activation, Concatenate, LSTM, GRU\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n",
    "from tensorflow.keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "import gc \n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_path = \"data/timeseries/\"\n",
    "\n",
    "x_train = pd.read_pickle(os.path.join(timeseries_path, \"x_train_lstm.p\"))\n",
    "x_dev = pd.read_pickle(os.path.join(timeseries_path, \"x_dev_lstm.p\"))\n",
    "x_test = pd.read_pickle(os.path.join(timeseries_path, \"x_test_lstm.p\"))\n",
    "\n",
    "y_train = pd.read_pickle(os.path.join(timeseries_path, \"y_train.p\"))\n",
    "y_dev = pd.read_pickle(os.path.join(timeseries_path, \"y_dev.p\"))\n",
    "y_test = pd.read_pickle(os.path.join(timeseries_path, \"y_test.p\"))\n",
    "\n",
    "ys = pd.read_pickle(os.path.join(timeseries_path, \"ys.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_path = \"data/drug_unique/smiles-transformer/\"\n",
    "\n",
    "x_train_drug = pd.read_pickle(os.path.join(drug_path, \"smiles_transformer_unique_train.p\"))\n",
    "x_dev_drug = pd.read_pickle(os.path.join(drug_path, \"smiles_transformer_unique_dev.p\"))\n",
    "x_test_drug = pd.read_pickle(os.path.join(drug_path, \"smiles_transformer_unique_test.p\"))\n",
    "\n",
    "sorted_x_train_drug = collections.OrderedDict(sorted(x_train_drug.items()))\n",
    "sorted_x_dev_drug = collections.OrderedDict(sorted(x_dev_drug.items()))\n",
    "sorted_x_test_drug = collections.OrderedDict(sorted(x_test_drug.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae96564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_data(sorted_data, dimension_size):\n",
    "    \n",
    "    drug_train_new = {}\n",
    "    for patient, representation in sorted_data.items():\n",
    "        len_rep = len(representation)\n",
    "        if  len_rep >= dimension_size:\n",
    "            new_representation = representation[:dimension_size]\n",
    "            drug_train_new[patient] = new_representation\n",
    "        else:\n",
    "            missing_vector_size = dimension_size - len_rep\n",
    "            new_representation = representation.copy()\n",
    "            for i in range(0, missing_vector_size):\n",
    "                temp = [0] * len(representation[0])\n",
    "                new_representation.append(temp)\n",
    "            drug_train_new[patient] = new_representation\n",
    "    return drug_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593235b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_drug_data(drug_size, train_drug_data, dev_drug_data,\n",
    "                     test_drug_data):\n",
    "    \n",
    "\n",
    "    drug_train_new = fill_data(train_drug_data, drug_size)\n",
    "    drug_dev_new = fill_data(dev_drug_data, drug_size)\n",
    "    drug_test_new = fill_data(test_drug_data, drug_size)\n",
    "\n",
    "    d_train, d_dev, d_test = [], [], []\n",
    "    for k,v in drug_train_new.items():\n",
    "        d_train.append(v)\n",
    "\n",
    "    for k,v in drug_dev_new.items():\n",
    "        d_dev.append(v)\n",
    "\n",
    "    for k,v in drug_test_new.items():\n",
    "        d_test.append(v)\n",
    "\n",
    "    d_train = np.asarray(d_train)\n",
    "    d_dev = np.asarray(d_dev)\n",
    "    d_test = np.asarray(d_test)\n",
    "    \n",
    "    return d_train, d_dev, d_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRUG_SIZE = 64\n",
    "train_drug, dev_drug, test_drug = create_drug_data(DRUG_SIZE, sorted_x_train_drug, sorted_x_dev_drug, sorted_x_test_drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras(model):\n",
    "    K.clear_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    gc.collect() # if it's done something you should see a number being outputted\n",
    "\n",
    "def make_prediction_multimodal(model, test_data):\n",
    "    probs = model.predict(test_data)\n",
    "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
    "    return probs, y_pred\n",
    "\n",
    "def save_scores_timeseries(predictions, probs, ground_truth):\n",
    "    \n",
    "    auc = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    result_dict = {}    \n",
    "    result_dict['auc'] = auc\n",
    "    result_dict['auprc'] = auprc\n",
    "    result_dict['acc'] = acc\n",
    "    result_dict['F1'] = F1\n",
    "\n",
    "\n",
    "    print(\"AUC: \", auc, \"AUPRC: \", auprc, \"ACC: \", acc, \"F1: \",F1)\n",
    "    return {\"auc\": auc,\n",
    "            \"auprc\": auprc,\n",
    "            \"acc\": acc,\n",
    "            \"F1\":F1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8af19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal(DRUG_SIZE):\n",
    "    input_img = Input(shape=(DRUG_SIZE, 1024), name = \"cnn\")\n",
    "    #encode_smiles = Embedding(input_dim=1024, output_dim=128, input_length=64)(input_img)\n",
    "    encode_smiles = Conv1D(filters=32, kernel_size=(3), activation='relu', padding='valid',  strides=1)(input_img)\n",
    "    encode_smiles = Conv1D(filters=64, kernel_size=(3),  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
    "    encode_smiles = Conv1D(filters=128, kernel_size=(3),  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
    "    encode_smiles = GlobalMaxPooling1D()(encode_smiles)\n",
    "    #encode_smiles = Flatten()(encode_smiles)\n",
    "\n",
    "    sequence_input = Input(shape=(24,104))\n",
    "    #sequence_input = Input(batch_size= (64, 24,104))\n",
    "    #sequence_input = \n",
    "    x = GRU(128)(sequence_input)\n",
    "\n",
    "    #batch_input_shape=(batch_size, n_timesteps, n_features), \n",
    "\n",
    "    #x = keras.layers.Concatenate()([x, text_embeddings])\n",
    "    x = tf.keras.layers.concatenate([x, encode_smiles])\n",
    "\n",
    "    # Fully connected \n",
    "    FC1 = Dense(1024, activation='relu')(x)\n",
    "    FC2 = Dropout(0.3)(FC1)\n",
    "    FC2 = Dense(512, activation='relu')(FC2)\n",
    "    FC2 = Dropout(0.3)(FC2)\n",
    "    FC2 = Dense(256, activation='relu')(FC2)\n",
    "\n",
    "    logits_regularizer = tf.keras.regularizers.L2(l2=0.05)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid',use_bias=True, \n",
    "                  #kernel_initializer=\"normal\",\n",
    "                    kernel_initializer=tf.keras.initializers.he_uniform(), \n",
    "                  kernel_regularizer=logits_regularizer\n",
    "\n",
    "                 )(FC2)\n",
    "\n",
    "    opt = Adam(lr=0.001, decay = 0.01)\n",
    "    #opt = Adam()\n",
    "    model = Model(inputs=[sequence_input, input_img], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91692fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 100\n",
    "model_patience = 5\n",
    "monitor_criteria = 'val_loss'\n",
    "batch_size = 32\n",
    "iteration_number = 10\n",
    "\n",
    "target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
    "\n",
    "save_scores = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[],}\n",
    "\n",
    "\n",
    "for iteration in range(0, iteration_number):\n",
    "    \n",
    "    np.random.seed(iteration)\n",
    "    tf.random.set_seed(iteration)\n",
    "    \n",
    "    K.clear_session()\n",
    "    temp_results = []\n",
    "    \n",
    "    for each_problem in target_problems:\n",
    "        print (\"Problem type: \", each_problem)\n",
    "        print (\"__________________\")\n",
    "\n",
    "\n",
    "        early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "        best_model_name = \"models/timeseries-smiles-transformer/smiles-transformer-multimodal_best_model_\"+str(iteration)+\".hdf5\"\n",
    "        checkpoint = ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,save_best_only=True, mode='min', period=1)\n",
    "\n",
    "\n",
    "        #cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n",
    "        #                                 save_weights_only = True, mode = 'min')\n",
    "\n",
    "\n",
    "        callbacks = [reduce_lr_loss, early_stopping_monitor, checkpoint]\n",
    "        ####################################\n",
    "        model = multimodal(DRUG_SIZE)\n",
    "        ####################################\n",
    "\n",
    "        if each_problem == \"mort_icu\":\n",
    "            class_weight_dict = {0: 1, 1: 5}\n",
    "        else: \n",
    "            class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                             np.unique(y_train[each_problem]),\n",
    "                                                             y_train[each_problem])  \n",
    "            class_weight_dict = dict(enumerate(class_weights))\n",
    "        \n",
    "        model.fit([x_train, train_drug], y_train[each_problem], epochs=epoch_num, verbose=1, \n",
    "                  validation_data=([x_dev, dev_drug], y_dev[each_problem]), callbacks=callbacks, \n",
    "                  batch_size= batch_size, class_weight=class_weight_dict)\n",
    "\n",
    "        model.load_weights(best_model_name)\n",
    "\n",
    "        probs, predictions = make_prediction_multimodal(model, [x_test, test_drug])\n",
    "        scores = save_scores_timeseries(predictions, probs, y_test[each_problem].values)\n",
    "        temp = {each_problem: scores}\n",
    "        temp_results.append(temp)\n",
    "        reset_keras(model)\n",
    "        try:\n",
    "            del model\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    save_scores[iteration] = temp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_result(data, target, metric):\n",
    "    res = 0\n",
    "    res_list = []\n",
    "    if target == \"mort_hosp\":\n",
    "        ind = 0\n",
    "    elif target == \"mort_icu\":\n",
    "        ind = 1\n",
    "    elif target == \"los_3\":\n",
    "        ind = 2\n",
    "    elif target == \"los_7\":\n",
    "        ind = 3\n",
    "    \n",
    "    counter = 0\n",
    "    for i,info in data.items():\n",
    "        #print(info[0])\n",
    "        if counter == 5:\n",
    "            break\n",
    "        counter +=1\n",
    "        res_list.append(info[ind][target][metric])\n",
    "        res += info[ind][target][metric]\n",
    "    \n",
    "    #print(target, metric, res / 5, np.mean(res_list), np.std(res_list))\n",
    "    print(target, metric,np.mean(res_list), np.std(res_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7eac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"mort_hosp\"\n",
    "compare_result(save_scores, target, \"auc\")\n",
    "compare_result(save_scores, target, \"auprc\")\n",
    "compare_result(save_scores, target, \"F1\")\n",
    "print(\"\")\n",
    "target = \"mort_icu\"\n",
    "compare_result(save_scores, target, \"auc\")\n",
    "compare_result(save_scores, target, \"auprc\")\n",
    "compare_result(save_scores, target, \"F1\")\n",
    "print(\"\")\n",
    "target = \"los_3\"\n",
    "compare_result(save_scores, target, \"auc\")\n",
    "compare_result(save_scores, target, \"auprc\")\n",
    "compare_result(save_scores, target, \"F1\")\n",
    "print(\"\")\n",
    "target = \"los_7\"\n",
    "compare_result(save_scores, target, \"auc\")\n",
    "compare_result(save_scores, target, \"auprc\")\n",
    "compare_result(save_scores, target, \"F1\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e029e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/timeseries-smiles-transformer/\"\n",
    "pd.to_pickle(save_scores, os.path.join(path, \"timeseries_smiles_transformer_unique_score.p\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
