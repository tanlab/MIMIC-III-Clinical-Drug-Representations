{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a03bce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d978f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np \n",
    "import keras\n",
    "#from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input, concatenate, Activation, Concatenate, LSTM, GRU\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n",
    "from tensorflow.keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "import gc \n",
    "import numpy as np\n",
    "import collections\n",
    "import shap\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc490761",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_path = \"data/timeseries/\"\n",
    "\n",
    "x_train = pd.read_pickle(os.path.join(timeseries_path, \"x_train_lstm.p\"))\n",
    "x_dev = pd.read_pickle(os.path.join(timeseries_path, \"x_dev_lstm.p\"))\n",
    "x_test = pd.read_pickle(os.path.join(timeseries_path, \"x_test_lstm.p\"))\n",
    "\n",
    "y_train = pd.read_pickle(os.path.join(timeseries_path, \"y_train.p\"))\n",
    "y_dev = pd.read_pickle(os.path.join(timeseries_path, \"y_dev.p\"))\n",
    "y_test = pd.read_pickle(os.path.join(timeseries_path, \"y_test.p\"))\n",
    "\n",
    "ys = pd.read_pickle(os.path.join(timeseries_path, \"ys.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4742deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_path = \"data/drug_unique/ecfp-1024/\"\n",
    "\n",
    "x_train_drug = pd.read_pickle(os.path.join(drug_path, \"ecfp_1024_unique_train.p\"))\n",
    "x_dev_drug = pd.read_pickle(os.path.join(drug_path, \"ecfp_1024_unique_dev.p\"))\n",
    "x_test_drug = pd.read_pickle(os.path.join(drug_path, \"ecfp_1024_unique_test.p\"))\n",
    "\n",
    "sorted_x_train_drug = collections.OrderedDict(sorted(x_train_drug.items()))\n",
    "sorted_x_dev_drug = collections.OrderedDict(sorted(x_dev_drug.items()))\n",
    "sorted_x_test_drug = collections.OrderedDict(sorted(x_test_drug.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eacf1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_data(sorted_data, dimension_size):\n",
    "    \n",
    "    drug_train_new = {}\n",
    "    for patient, representation in sorted_data.items():\n",
    "        len_rep = len(representation)\n",
    "        if  len_rep >= dimension_size:\n",
    "            new_representation = representation[:dimension_size]\n",
    "            drug_train_new[patient] = new_representation\n",
    "        else:\n",
    "            missing_vector_size = dimension_size - len_rep\n",
    "            new_representation = representation.copy()\n",
    "            for i in range(0, missing_vector_size):\n",
    "                temp = [0] * len(representation[0])\n",
    "                new_representation.append(temp)\n",
    "            drug_train_new[patient] = new_representation\n",
    "    return drug_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8bb3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_drug_data(drug_size, train_drug_data, dev_drug_data,\n",
    "                     test_drug_data):\n",
    "    \n",
    "\n",
    "    drug_train_new = fill_data(train_drug_data, drug_size)\n",
    "    drug_dev_new = fill_data(dev_drug_data, drug_size)\n",
    "    drug_test_new = fill_data(test_drug_data, drug_size)\n",
    "\n",
    "    d_train, d_dev, d_test = [], [], []\n",
    "    for k,v in drug_train_new.items():\n",
    "        d_train.append(v)\n",
    "\n",
    "    for k,v in drug_dev_new.items():\n",
    "        d_dev.append(v)\n",
    "\n",
    "    for k,v in drug_test_new.items():\n",
    "        d_test.append(v)\n",
    "\n",
    "    d_train = np.asarray(d_train)\n",
    "    d_dev = np.asarray(d_dev)\n",
    "    d_test = np.asarray(d_test)\n",
    "    \n",
    "    return d_train, d_dev, d_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b79b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRUG_SIZE = 64\n",
    "train_drug, dev_drug, test_drug = create_drug_data(DRUG_SIZE, sorted_x_train_drug, sorted_x_dev_drug, sorted_x_test_drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff6f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras(model):\n",
    "    K.clear_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    gc.collect() # if it's done something you should see a number being outputted\n",
    "\n",
    "def make_prediction_multimodal(model, test_data):\n",
    "    probs = model.predict(test_data)\n",
    "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
    "    return probs, y_pred\n",
    "\n",
    "def save_scores_timeseries(predictions, probs, ground_truth):\n",
    "    \n",
    "    auc = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    result_dict = {}    \n",
    "    result_dict['auc'] = auc\n",
    "    result_dict['auprc'] = auprc\n",
    "    result_dict['acc'] = acc\n",
    "    result_dict['F1'] = F1\n",
    "\n",
    "\n",
    "    print(\"AUC: \", auc, \"AUPRC: \", auprc, \"ACC: \", acc, \"F1: \",F1)\n",
    "    return {\"auc\": auc,\n",
    "            \"auprc\": auprc,\n",
    "            \"acc\": acc,\n",
    "            \"F1\":F1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22c9b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal(DRUG_SIZE):\n",
    "    input_img = Input(shape=(DRUG_SIZE, 1024), name = \"cnn\")\n",
    "    #encode_smiles = Embedding(input_dim=1024, output_dim=128, input_length=64)(input_img)\n",
    "    encode_smiles = Conv1D(filters=32, kernel_size=(3), activation='relu', padding='valid',  strides=1)(input_img)\n",
    "    encode_smiles = Conv1D(filters=64, kernel_size=(3),  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
    "    encode_smiles = Conv1D(filters=128, kernel_size=(3),  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
    "    encode_smiles = GlobalMaxPooling1D()(encode_smiles)\n",
    "    #encode_smiles = Flatten()(encode_smiles)\n",
    "\n",
    "    sequence_input = Input(shape=(24,104))\n",
    "    #sequence_input = Input(batch_size= (64, 24,104))\n",
    "    #sequence_input = \n",
    "    x = GRU(128)(sequence_input)\n",
    "\n",
    "    #batch_input_shape=(batch_size, n_timesteps, n_features), \n",
    "\n",
    "    #x = keras.layers.Concatenate()([x, text_embeddings])\n",
    "    x = tf.keras.layers.concatenate([x, encode_smiles])\n",
    "\n",
    "    # Fully connected \n",
    "    FC1 = Dense(1024, activation='relu')(x)\n",
    "    FC2 = Dropout(0.3)(FC1)\n",
    "    FC2 = Dense(512, activation='relu')(FC2)\n",
    "    FC2 = Dropout(0.3)(FC2)\n",
    "    FC2 = Dense(256, activation='relu')(FC2)\n",
    "\n",
    "    logits_regularizer = tf.keras.regularizers.L2(l2=0.05)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid',use_bias=True, \n",
    "                  #kernel_initializer=\"normal\",\n",
    "                    kernel_initializer=tf.keras.initializers.he_uniform(), \n",
    "                  kernel_regularizer=logits_regularizer\n",
    "\n",
    "                 )(FC2)\n",
    "\n",
    "    opt = Adam(lr=0.001, decay = 0.01)\n",
    "    #opt = Adam()\n",
    "    model = Model(inputs=[sequence_input, input_img], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d7c07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 00:18:42.461021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:42.468407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:42.468971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 00:18:42.470421: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-13 00:18:42.470857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:42.471374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:42.471868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:42.779126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:42.779455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:42.779738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:42.780003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8830 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "Pass classes=[0 1], y=subject_id  hadm_id  icustay_id\n",
      "6           107064   228232        0\n",
      "9           150750   220597        1\n",
      "11          194540   229441        0\n",
      "12          112213   232669        1\n",
      "13          143045   263738        0\n",
      "                                  ..\n",
      "99966       167228   252173        0\n",
      "99973       150202   275083        0\n",
      "99982       151454   221194        0\n",
      "99991       151118   226241        0\n",
      "99995       137810   229633        0\n",
      "Name: mort_hosp, Length: 15182, dtype: category\n",
      "Categories (2, int64): [0, 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15182 samples, validate on 2164 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 00:18:43.253056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:43.253455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:43.253768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:43.254106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:43.254414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 00:18:43.254694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8830 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 00:18:44.019324: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15182/15182 [==============================] - ETA: 0s - loss: 0.5537 - acc: 0.7305"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57297, saving model to models/timeseries-ecfp/unique/temp0.hdf5\n",
      "15182/15182 [==============================] - 14s 893us/sample - loss: 0.5537 - acc: 0.7305 - val_loss: 0.5730 - val_acc: 0.7075 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "15168/15182 [============================>.] - ETA: 0s - loss: 0.4512 - acc: 0.7799\n",
      "Epoch 00002: val_loss improved from 0.57297 to 0.45485, saving model to models/timeseries-ecfp/unique/temp0.hdf5\n",
      "15182/15182 [==============================] - 12s 765us/sample - loss: 0.4510 - acc: 0.7801 - val_loss: 0.4548 - val_acc: 0.7777 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "15168/15182 [============================>.] - ETA: 0s - loss: 0.4120 - acc: 0.8006\n",
      "Epoch 00003: val_loss did not improve from 0.45485\n",
      "15182/15182 [==============================] - 10s 681us/sample - loss: 0.4119 - acc: 0.8006 - val_loss: 0.4562 - val_acc: 0.7907 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "15168/15182 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8186\n",
      "Epoch 00004: val_loss improved from 0.45485 to 0.44125, saving model to models/timeseries-ecfp/unique/temp0.hdf5\n",
      "15182/15182 [==============================] - 12s 780us/sample - loss: 0.3815 - acc: 0.8187 - val_loss: 0.4412 - val_acc: 0.7907 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "15168/15182 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8343\n",
      "Epoch 00005: val_loss did not improve from 0.44125\n",
      "15182/15182 [==============================] - 11s 717us/sample - loss: 0.3530 - acc: 0.8343 - val_loss: 0.4518 - val_acc: 0.7893 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "15168/15182 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.8451\n",
      "Epoch 00006: val_loss improved from 0.44125 to 0.42579, saving model to models/timeseries-ecfp/unique/temp0.hdf5\n",
      "15182/15182 [==============================] - 13s 835us/sample - loss: 0.3318 - acc: 0.8451 - val_loss: 0.4258 - val_acc: 0.8073 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "15182/15182 [==============================] - ETA: 0s - loss: 0.3087 - acc: 0.8581\n",
      "Epoch 00007: val_loss did not improve from 0.42579\n",
      "15182/15182 [==============================] - 12s 778us/sample - loss: 0.3087 - acc: 0.8581 - val_loss: 0.4383 - val_acc: 0.8055 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "15182/15182 [==============================] - ETA: 0s - loss: 0.2876 - acc: 0.8704\n",
      "Epoch 00008: val_loss improved from 0.42579 to 0.40636, saving model to models/timeseries-ecfp/unique/temp0.hdf5\n",
      "15182/15182 [==============================] - 12s 790us/sample - loss: 0.2876 - acc: 0.8704 - val_loss: 0.4064 - val_acc: 0.8226 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "15136/15182 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.8778\n",
      "Epoch 00009: val_loss improved from 0.40636 to 0.38771, saving model to models/timeseries-ecfp/unique/temp0.hdf5\n",
      "15182/15182 [==============================] - 13s 843us/sample - loss: 0.2734 - acc: 0.8776 - val_loss: 0.3877 - val_acc: 0.8369 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "15136/15182 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.8890\n",
      "Epoch 00010: val_loss did not improve from 0.38771\n",
      "15182/15182 [==============================] - 11s 734us/sample - loss: 0.2550 - acc: 0.8887 - val_loss: 0.3971 - val_acc: 0.8323 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "15182/15182 [==============================] - ETA: 0s - loss: 0.2449 - acc: 0.8935\n",
      "Epoch 00011: val_loss did not improve from 0.38771\n",
      "15182/15182 [==============================] - 13s 835us/sample - loss: 0.2449 - acc: 0.8935 - val_loss: 0.4119 - val_acc: 0.8281 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "15136/15182 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9015\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.38771\n",
      "15182/15182 [==============================] - 11s 752us/sample - loss: 0.2291 - acc: 0.9016 - val_loss: 0.4062 - val_acc: 0.8336 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "15136/15182 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9079\n",
      "Epoch 00013: val_loss did not improve from 0.38771\n",
      "15182/15182 [==============================] - 13s 834us/sample - loss: 0.2202 - acc: 0.9078 - val_loss: 0.4038 - val_acc: 0.8355 - lr: 1.0000e-04\n",
      "AUC:  0.8736668184944048 AUPRC:  0.5245127825638716 ACC:  0.8476058931860037 F1:  0.47039999999999993\n"
     ]
    }
   ],
   "source": [
    "#epoch_num = 100\n",
    "epoch_num = 100\n",
    "model_patience = 4\n",
    "monitor_criteria = 'val_loss'\n",
    "batch_size = 32\n",
    "iteration_number = 10\n",
    "\n",
    "target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
    "\n",
    "save_scores = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[],}\n",
    "\n",
    "\n",
    "for iteration in range(0, iteration_number):\n",
    "    np.random.seed(iteration)\n",
    "    tf.random.set_seed(iteration)\n",
    "    \n",
    "    K.clear_session()\n",
    "    temp_results = []\n",
    "    \n",
    "    for each_problem in target_problems:\n",
    "        print (\"Problem type: \", each_problem)\n",
    "        print (\"__________________\")\n",
    "\n",
    "\n",
    "        early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "        best_model_name = \"models/timeseries-ecfp/ecfp-multimodal_best_model_\"+str(iteration)+\".hdf5\"\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,save_best_only=True, mode='min', period=1)\n",
    "\n",
    "\n",
    "        #cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n",
    "        #                                 save_weights_only = True, mode = 'min')\n",
    "\n",
    "\n",
    "        callbacks = [reduce_lr_loss, early_stopping_monitor, checkpoint]\n",
    "        ####################################\n",
    "        model = multimodal(DRUG_SIZE)\n",
    "        ####################################\n",
    "\n",
    "        if each_problem == \"mort_icu\":\n",
    "            class_weight_dict = {0: 1, 1: 5}\n",
    "        else: \n",
    "            class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                             np.unique(y_train[each_problem]),\n",
    "                                                             y_train[each_problem])  \n",
    "            class_weight_dict = dict(enumerate(class_weights))\n",
    "#         class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                          np.unique(y_train[each_problem]),\n",
    "#                                                          y_train[each_problem])\n",
    "            \n",
    "        \n",
    "        model.fit([x_train, train_drug], y_train[each_problem], epochs=epoch_num, verbose=1, \n",
    "                  validation_data=([x_dev, dev_drug], y_dev[each_problem]), callbacks=callbacks, \n",
    "                  batch_size= batch_size, class_weight=class_weight_dict)\n",
    "\n",
    "        model.load_weights(best_model_name)\n",
    "\n",
    "        probs, predictions = make_prediction_multimodal(model, [x_test, test_drug])\n",
    "        scores = save_scores_timeseries(predictions, probs, y_test[each_problem].values)\n",
    "        temp = {each_problem: scores}\n",
    "        temp_results.append(temp)\n",
    "        break\n",
    "        reset_keras(model)\n",
    "        try:\n",
    "            del model\n",
    "        except:\n",
    "            pass\n",
    "    break\n",
    "    save_scores[iteration] = temp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc08249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_result(data, target, metric):\n",
    "    res = 0\n",
    "    res_list = []\n",
    "    if target == \"mort_hosp\":\n",
    "        ind = 0\n",
    "    elif target == \"mort_icu\":\n",
    "        ind = 1\n",
    "    elif target == \"los_3\":\n",
    "        ind = 2\n",
    "    elif target == \"los_7\":\n",
    "        ind = 3\n",
    "    \n",
    "    counter = 0\n",
    "    for i,info in data.items():\n",
    "        #print(info[0])\n",
    "        if counter == 5:\n",
    "            break\n",
    "        counter +=1\n",
    "        res_list.append(info[ind][target][metric])\n",
    "        res += info[ind][target][metric]\n",
    "    \n",
    "    #print(target, metric, res / 5, np.mean(res_list), np.std(res_list))\n",
    "    print(target, metric,np.mean(res_list), np.std(res_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb25b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"mort_hosp\"\n",
    "compare_result(save_scores, target, \"auc\")\n",
    "compare_result(save_scores, target, \"auprc\")\n",
    "compare_result(save_scores, target, \"F1\")\n",
    "print(\"\")\n",
    "target = \"mort_icu\"\n",
    "compare_result(save_scores, target, \"auc\")\n",
    "compare_result(save_scores, target, \"auprc\")\n",
    "compare_result(save_scores, target, \"F1\")\n",
    "print(\"\")\n",
    "target = \"los_3\"\n",
    "compare_result(save_scores, target, \"auc\")\n",
    "compare_result(save_scores, target, \"auprc\")\n",
    "compare_result(save_scores, target, \"F1\")\n",
    "print(\"\")\n",
    "target = \"los_7\"\n",
    "compare_result(save_scores, target, \"auc\")\n",
    "compare_result(save_scores, target, \"auprc\")\n",
    "compare_result(save_scores, target, \"F1\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfbc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/timeseries-ecfp-unique/\"\n",
    "pd.to_pickle(save_scores, os.path.join(path, \"timeseries_ecfp_unique_score.p\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
